\documentclass{article}

\begin{document}
<<loadpackages, echo=FALSE, warning=FALSE, message=FALSE>>=
library(durhamevp)
library(tidyverse)
@

We only have access to a big data set via boolean search.

Sub-sets of the data set returned by a single key word on a single day are large.For example for the single search term `election' for the single day of 1 August 1868 returns 1982 articles. The search term `riot' for the same day (close to the 1868 General Election) returns 360 articles, 228 of which do not also contain the word election.

With even a small number of search terms it quickly becomes impractical to examine all the documents even for a single day.

Following King et al we define
$S$ - the search set of all documents in the British Newspaper Archive
$T$ - the target set of all documents in the British Newspaper Archive which are about election violence
$R$ - a reference set of documents which are about election violence

The task is to identify $T$ from $S$ in a form where $T$ can be

It is trivial to define an algorithm which obtains a subset of $S$ which contains $T$, because $S \subseteq S$ and $T \subset S$. Algorithms which aim to maximise the chances of obtaining all of $T$ will tend to return $S$.

Our task is to find a good method for returning $T$ from $S$ in a form which we can analyse. By a \textit{good} method we mean a method which returns a greater proportion of $T$, and a greater ratio of $T$ to $\neg T$ than alternative methods. The main alternative method is manual searching by historians.

\section{The data}
<<loaddata, cache=TRUE, tidy=TRUE>>=
classdocs<-durhamevp::get_classified_docs()
# just some candidate document for speed/space reasons
unclassdocs<-durhamevp::get_candidate_documents(3000:6000)

@

In the data there are \Sexpr{nrow(classdocs)} cases:
\begin{itemize}
  \item \textbf{\Sexpr{sum(classdocs$election_article==0)}} non-election articles.
  \item \textbf{\Sexpr{sum(classdocs$EV_article==1)}} election violence articles.
  \item \textbf{\Sexpr{sum(classdocs$just_election==1)}} election (but not violent) articles.
\end{itemize}

\section{Keyword Identification}
Note it is important to make keyword identification somewhat selective of $T$ from $S$ otherwise even very good stage 2 \& 3 selection processes the false positives will overwhelm the true positives.

Algorithm:
\begin{enumerate}
  \item use classifier on $R$ and $S$ to identify two lists of keywords
  \item generate probability from classifier parameters
  \item add to keyword list based on probability
\end{enumerate}
<<findkeywords>>=
classified_corpus<-quanteda::corpus(classdocs[,c("fakeid", "ocr", "EV_article")], text_field = "ocr")
classified_dfm <- preprocess_corpus(classified_corpus, stem=FALSE, min_termfreq=20, min_docfreq = 10)

keywords<-nb_keywords(classified_dfm, "EV_article")
knitr::kable(head(keywords, 20))
@

\section{Refinement Using Description}


<<usingdescription, warning=FALSE>>=
description_corpus<-quanteda::corpus(classdocs[,c("fakeid", "description", "EV_article")], text_field = "description")
description_dfm <- preprocess_corpus(description_corpus, stem=FALSE, min_termfreq=5, min_docfreq = 2)

both_dfms<-split_dfm(description_dfm, n_train = 1000)

nb<-quanteda::textmodel_nb(both_dfms$train, y=quanteda::docvars(both_dfms$train, "EV_article"), prior="uniform")
prob_nb<-predict(nb, newdata = both_dfms$test, type="probability")
pred_nb<-data.frame(predict(nb, newdata = both_dfms$test, type="class"))
res<-data.frame(predict_nb=pred_nb, prob_nb)
names(res)<-c("predict_nb", "prob_notev", "prob_ev")
assess_classification(organise_results(both_dfms$test, res)) %>%
  filter(rowname %in% c("Precision", "Recall", "F1")) %>%
  kable()

description_corpus<-quanteda::corpus(classdocs[,c("fakeid", "description", "EV_article")], text_field = "description")
description_dfm <- preprocess_corpus(description_corpus, stem=FALSE, min_termfreq=5, min_docfreq = 2)

both_dfms<-split_dfm(description_dfm, n_train = 1000)

@


Note: this way of creating the dfm does make dfms terms equal because one overall dfm is created and then it is subset. You can see below that the number of features in both dfms is the same:

<<equaldfms>>=
print(both_dfms$train)
print(both_dfms$test)
@

\subsection{More realistic use case}
Exclude non-election cases which would not be returned by our keywords:


<<usingdescriptionrealistic, warning=FALSE>>=
description_corpus<-quanteda::corpus(classdocs[classdocs$election_article==1,c("fakeid", "description", "EV_article")], text_field = "description")
description_dfm <- preprocess_corpus(description_corpus, stem=FALSE, min_termfreq=5, min_docfreq = 2)

both_dfms<-split_dfm(description_dfm, n_train = 1000)

nb<-quanteda::textmodel_nb(both_dfms$train, y=quanteda::docvars(both_dfms$train, "EV_article"), prior="uniform")
prob_nb<-predict(nb, newdata = both_dfms$test, type="probability")
pred_nb<-data.frame(predict(nb, newdata = both_dfms$test, type="class"))
res<-data.frame(predict_nb=pred_nb, prob_nb)
names(res)<-c("predict_nb", "prob_notev", "prob_ev")
assess_classification(organise_results(both_dfms$test, res)) %>%
  filter(rowname %in% c("Precision", "Recall", "F1")) %>%
  kable()

description_corpus<-quanteda::corpus(classdocs[,c("fakeid", "description", "EV_article")], text_field = "description")
description_dfm <- preprocess_corpus(description_corpus, stem=FALSE, min_termfreq=5, min_docfreq = 2)

both_dfms<-split_dfm(description_dfm, n_train = 1000)

@

\section{Refinement Using OCR}

<<usingocr>>=
ocr_corpus<-quanteda::corpus(classdocs[,c("fakeid", "ocr", "EV_article")], text_field = "ocr")
ocr_dfm <- preprocess_corpus(ocr_corpus, stem=FALSE, min_termfreq=5, min_docfreq = 2)

both_dfms<-split_dfm(ocr_dfm, n_train = 1000)
nb<-quanteda::textmodel_nb(both_dfms$train, y=quanteda::docvars(both_dfms$train, "EV_article"), prior="uniform")
prob_nb<-predict(nb, newdata = both_dfms$test, type="probability")
pred_nb<-data.frame(predict(nb, newdata = both_dfms$test, type="class"))
res<-data.frame(predict_nb=pred_nb, prob_nb)
names(res)<-c("predict_nb", "prob_notev", "prob_ev")
assess_classification(organise_results(both_dfms$test, res)) %>%
  filter(rowname %in% c("Precision", "Recall", "F1")) %>%
  kable()
@


\section{King Algorithm: implementation}

\subsection{Incrementally Define $R$ and $S$}
$R$ is our reference set. [King suggestions: could define $R$ based on one simple keyword search].
\subsubsection{Intermediate step}
Take keywords in $R$, $K_R$, ranked by simple statistic such as document frequency or frequency-inverse document frequency. User examines elements of $K_R$ apart from those used to define the set and chooses some keywords to define $Q_S$, which in turn generates a definition for $S$ so that we can run the rest of the algorithm.
The user can continue to add keywords from $K_R$ into the final desired query $Q_RT$.
This step also mitigates the issue of how to define a search set in large data sets that do not fit into memory all at once or may not even be able to be retrieved all at onece. is the BNA.

\subsection{Partition $S$ into $T$ and $S \backslash T$}
To partition $S$ into $T$ and $S \backslash T$, we first define a `training' set by sampling from $S$ and $R$. Since $R$ is typically much smaller than $S$ our test set for our classifiers is all all of $S$, we often use the entire $R$ set and a sample of $S$ as our training set.
<<fitkingtrain>>=
R <- classdocs[classdocs$EV_article==1,]
R$R <- 1
R$in_sample<-1
S <- unclassdocs
S$R <- 0

n_sample_S <- 1000
S$in_sample<-0
S$in_sample[sample(1:nrow(S), n_sample_S)]<-1

R_S <- dplyr::bind_rows(R, S)
R_S$fakeid<-1:nrow(R_S)



R_S_corpus<-quanteda::corpus(R_S[,c("fakeid", "ocr", "in_sample", "R")], text_field = "ocr")
R_S_dfm <- preprocess_corpus(R_S_corpus, stem=FALSE, min_termfreq=20, min_docfreq = 20)

king_train_dfm<-quanteda::dfm_subset(R_S_dfm, quanteda::docvars(R_S_dfm, "in_sample")==1)
king_nb <- quanteda::textmodel_nb(king_train_dfm, y=quanteda::docvars(king_train_dfm, "R"), prior="uniform")
keywords<-nb_keywords(king_train_dfm, "R")
knitr::kable(head(keywords, 20))
@

After fitting the classifiers, we use the estimated parameters to generate predicted probabilities of $R$ membership for all documents in $S$. Of cource, all the search set documents in fact fall within $S$ but our interest in in learning from the \textit{mistakes} these classifiers make.

<<learnclassmistakes>>=
S_dfm <- quanteda::dfm_subset(R_S_dfm, quanteda::docvars(R_S_dfm, "R")==0)
quanteda::docvars(S_dfm, "T")<-predict(king_nb, newdata = S_dfm, type="class")
@

\subsection{Discovering Keywords to Classify Documents}
After partitioning $S$ into estimated target $T$ and non-target set $S \backslash T$, we must find and rank keywords which best discriminate $T$ and $S \backslash T$.
King does this in three stages:
\subsubsection{identify keywords in S}

\subsubsection{sort them into those that predict each of the two steps}

My suggestion here is simply to use binary\_dfm

<<>>=
S_dfm_binary<-quanteda::dfm_weight(S_dfm, scheme="boolean")
king_stage2 <- quanteda::textmodel_nb(S_dfm, y=quanteda::docvars(S_dfm_binary, "T"), distribution="Bernoulli")
king_stage2 <- nb_keywords(S_dfm, "T")
knitr::kable(head(king_stage2, 30))

knitr::kable(tail(king_stage2, 30))
@

\subsection{rank them by degree of discriminatory power}
We already seem to have done this in the step above
\subsection{Human Input and Human-Computer Iteration}
Now present


\end{document}
