\documentclass{article}
\usepackage{amsmath}

\begin{document}
<<loadpackages, echo=FALSE, warning=FALSE, message=FALSE>>=
library(durhamevp)
library(tidyverse)
@

We only have access to a big data set via boolean search.

Sub-sets of the data set returned by a single key word on a single day are large.For example for the single search term `election' for the single day of 1 August 1868 returns 1982 articles. The search term `riot' for the same day (close to the 1868 General Election) returns 360 articles, 228 of which do not also contain the word election.

With even a small number of search terms it quickly becomes impractical to examine all the documents even for a single day.

Following King et al we define
$S$ - the search set of all documents in the British Newspaper Archive
$T$ - the target set of all documents in the British Newspaper Archive which are about election violence
$R$ - a reference set of documents which are about election violence

The task is to identify $T$ from $S$ in a form where $T$ can be

It is trivial to define an algorithm which obtains a subset of $S$ which contains $T$, because $S \subseteq S$ and $T \subset S$. Algorithms which aim to maximise the chances of obtaining all of $T$ will tend to return $S$.

Our task is to find a good method for returning $T$ from $S$ in a form which we can analyse. By a \textit{good} method we mean a method which returns a greater proportion of $T$, and a greater ratio of $T$ to $\neg T$ than alternative methods. The main alternative method is manual searching by historians.

\section{The data}
<<loaddata, cache=TRUE, tidy=TRUE>>=
classdocs<-durhamevp::get_classified_docs()

# just some candidate document for speed/space reasons
unclassdocs<-durhamevp::get_candidate_documents(3000:6000)
@

In the classified data there are \Sexpr{nrow(classdocs)} cases:
\begin{itemize}
  \item \textbf{\Sexpr{sum(classdocs$election_article==0)}} non-election articles.
  \item \textbf{\Sexpr{sum(classdocs$EV_article==1)}} election violence articles.
  \item \textbf{\Sexpr{sum(classdocs$just_election==1)}} election (but not violent) articles.
\end{itemize}

\section{Keyword Identification}
Note it is important to make keyword identification somewhat selective of $T$ from $S$ otherwise even very good stage 2 \& 3 selection processes the false positives will overwhelm the true positives.

Algorithm:
\begin{enumerate}
  \item use classifier on $R$ and $S$ to identify two lists of keywords
  \item generate probability from classifier parameters
  \item add to keyword list based on probability
\end{enumerate}
<<findkeywords>>=
classified_corpus<-quanteda::corpus(classdocs[,c("fakeid", "ocr", "EV_article")], text_field = "ocr")
classified_dfm <- preprocess_corpus(classified_corpus, stem=FALSE, min_termfreq=20, min_docfreq = 10)

keywords<-nb_keywords(classified_dfm, "EV_article")
knitr::kable(head(keywords, 20))
@

\section{Refinement Using Description}


<<usingdescription, warning=FALSE>>=
description_corpus<-quanteda::corpus(classdocs[,c("fakeid", "description", "EV_article")], text_field = "description")
description_dfm <- preprocess_corpus(description_corpus, stem=FALSE, min_termfreq=5, min_docfreq = 2)

both_dfms<-split_dfm(description_dfm, n_train = 700)

nb<-quanteda::textmodel_nb(both_dfms$train, y=quanteda::docvars(both_dfms$train, "EV_article"), prior="uniform")
prob_nb<-predict(nb, newdata = both_dfms$test, type="probability")
pred_nb<-data.frame(predict(nb, newdata = both_dfms$test, type="class"))
res<-data.frame(predict_nb=pred_nb, prob_nb)
names(res)<-c("predict_nb", "prob_notev", "prob_ev")
assess_classification(organise_results(both_dfms$test, res)) %>%
  filter(rowname %in% c("Precision", "Recall", "F1")) %>%
  kable()

description_corpus<-quanteda::corpus(classdocs[,c("fakeid", "description", "EV_article")], text_field = "description")
description_dfm <- preprocess_corpus(description_corpus, stem=FALSE, min_termfreq=5, min_docfreq = 2)

both_dfms<-split_dfm(description_dfm, n_train = 700)

@


Note: this way of creating the dfm does make dfms terms equal because one overall dfm is created and then it is subset. You can see below that the number of features in both dfms is the same:

<<equaldfms>>=
print(both_dfms$train)
print(both_dfms$test)
@

\subsection{More realistic use case}
Exclude non-election cases which would not be returned by our keywords:


<<usingdescriptionrealistic, warning=FALSE>>=
description_corpus<-quanteda::corpus(classdocs[classdocs$election_article==1,c("fakeid", "description", "EV_article")], text_field = "description")
description_dfm <- preprocess_corpus(description_corpus, stem=FALSE, min_termfreq=5, min_docfreq = 2)

both_dfms<-split_dfm(description_dfm, n_train = 700)

nb<-quanteda::textmodel_nb(both_dfms$train, y=quanteda::docvars(both_dfms$train, "EV_article"), prior="uniform")
prob_nb<-predict(nb, newdata = both_dfms$test, type="probability")
pred_nb<-data.frame(predict(nb, newdata = both_dfms$test, type="class"))
res<-data.frame(predict_nb=pred_nb, prob_nb)
names(res)<-c("predict_nb", "prob_notev", "prob_ev")
assess_classification(organise_results(both_dfms$test, res)) %>%
  filter(rowname %in% c("Precision", "Recall", "F1")) %>%
  kable()

description_corpus<-quanteda::corpus(classdocs[,c("fakeid", "description", "EV_article")], text_field = "description")
description_dfm <- preprocess_corpus(description_corpus, stem=FALSE, min_termfreq=5, min_docfreq = 2)

both_dfms<-split_dfm(description_dfm, n_train = 1000)

@

\section{Refinement Using OCR}

<<usingocr>>=
ocr_corpus<-quanteda::corpus(classdocs[,c("fakeid", "ocr", "EV_article")], text_field = "ocr")
ocr_dfm <- preprocess_corpus(ocr_corpus, stem=FALSE, min_termfreq=5, min_docfreq = 2)

both_dfms<-split_dfm(ocr_dfm, n_train = 700)
nb<-quanteda::textmodel_nb(both_dfms$train, y=quanteda::docvars(both_dfms$train, "EV_article"), prior="uniform")
prob_nb<-predict(nb, newdata = both_dfms$test, type="probability")
pred_nb<-data.frame(predict(nb, newdata = both_dfms$test, type="class"))
res<-data.frame(predict_nb=pred_nb, prob_nb)
names(res)<-c("predict_nb", "prob_notev", "prob_ev")
assess_classification(organise_results(both_dfms$test, res)) %>%
  filter(rowname %in% c("Precision", "Recall", "F1")) %>%
  kable()
@


\section{King Algorithm: implementation}

\subsection{Incrementally Define $R$ and $S$}
$R$ is our reference set. [King suggestions: could define $R$ based on one simple keyword search].
\subsubsection{Intermediate step}
Take keywords in $R$, $K_R$, ranked by simple statistic such as document frequency or frequency-inverse document frequency. User examines elements of $K_R$ apart from those used to define the set and chooses some keywords to define $Q_S$, which in turn generates a definition for $S$ so that we can run the rest of the algorithm.
The user can continue to add keywords from $K_R$ into the final desired query $Q_RT$.
This step also mitigates the issue of how to define a search set in large data sets that do not fit into memory all at once or may not even be able to be retrieved all at onece. is the BNA.

\subsection{Partition $S$ into $T$ and $S \backslash T$}
To partition $S$ into $T$ and $S \backslash T$, we first define a `training' set by sampling from $S$ and $R$. Since $R$ is typically much smaller than $S$ our test set for our classifiers is all all of $S$, we often use the entire $R$ set and a sample of $S$ as our training set.
<<fitkingtrain>>=
R <- classdocs[classdocs$EV_article==1,]
R$R <- 1
R$in_sample<-1
S <- unclassdocs
S$R <- 0

n_sample_S <- 700
S$in_sample<-0
S$in_sample[sample(1:nrow(S), n_sample_S)]<-1

R_S <- dplyr::bind_rows(R, S)
R_S$fakeid<-1:nrow(R_S)



R_S_corpus<-quanteda::corpus(R_S[,c("fakeid", "ocr", "in_sample", "R")], text_field = "ocr")
R_S_dfm <- preprocess_corpus(R_S_corpus, stem=FALSE, min_termfreq=20, min_docfreq = 20)

king_train_dfm<-quanteda::dfm_subset(R_S_dfm, quanteda::docvars(R_S_dfm, "in_sample")==1)
king_nb <- quanteda::textmodel_nb(king_train_dfm, y=quanteda::docvars(king_train_dfm, "R"), prior="uniform")
keywords<-nb_keywords(king_train_dfm, "R")
knitr::kable(head(keywords, 20))
@

After fitting the classifiers, we use the estimated parameters to generate predicted probabilities of $R$ membership for all documents in $S$. Of cource, all the search set documents in fact fall within $S$ but our interest in in learning from the \textit{mistakes} these classifiers make.

<<learnclassmistakes>>=
S_dfm <- quanteda::dfm_subset(R_S_dfm, quanteda::docvars(R_S_dfm, "R")==0)
quanteda::docvars(S_dfm, "T")<-predict(king_nb, newdata = S_dfm, type="class")
@

\subsection{Discovering Keywords to Classify Documents}
After partitioning $S$ into estimated target $T$ and non-target set $S \backslash T$, we must find and rank keywords which best discriminate $T$ and $S \backslash T$.
King does this in three stages:
\subsubsection{identify keywords in S}

\subsubsection{sort them into those that predict each of the two steps}

My suggestion here is simply to use binary\_dfm

<<>>=
S_dfm_binary<-quanteda::dfm_weight(S_dfm, scheme="boolean")
king_stage2 <- quanteda::textmodel_nb(S_dfm, y=quanteda::docvars(S_dfm_binary, "T"), distribution="Bernoulli")
king_stage2 <- nb_keywords(S_dfm, "T")
knitr::kable(head(king_stage2, 30))

knitr::kable(tail(king_stage2, 30))
@

\subsection{rank them by degree of discriminatory power}
We already seem to have done this in the step above
\subsection{Human Input and Human-Computer Iteration}

The King et al suggestion is to present the above lists to humans who then choose additional keywords from these lists. That might be fine if the final goal is develop keywords. However, this does not work so well if the final goal is to obtain sets of documents based on those keywords. Some problems with this approach for obtaining documents
\begin{enumerate}
  \item the set of documents will keep on growing without limit (with many irrelevant documents)
  \item there is no guidance about which keywords to select from the lists presented
\end{enumerate}

An alternative proposal is to continue with a classification approach. That is to treat the classifer as informative about two separate matters: keywords retrival as potentially informative

The stage II classifier is informative about the following issues:
\begin{enumerate}
  \item terms which are in $S$ but not in $R$ which may be positively predictive of election violence.
  \item terms which are very differently predictive of election violence between stage I and stage II classification.
  \item
\end{enumerate}

Idea 1: For keyword $t$ in keywords $1 \dots n$, $Q_t$ is the Boolean query which returns document $d$ from $S$ if $d$ contains $t$. $S\prime$ is the set of documents returned by the set of queries $Q_{T_1} \dots Q_{T_n}$. The probability that document $d$ is an example of the concept of interest is given by the set of queries which return that document. We run independent queries with each of the keywords. We have different sets returned from

Idea 2: King's stage II classification can be thought of of the first part of an EM process which augments information from labelled data with information from unlabelled data. See for a classic example Nigam et. al. 2000 - Semi-supervised parameter estimation: can learn from a combination of labeled and unlabeled data in a loop. First train a classifier using available labeled documents, and probabilistically label the unlabeled documents. Then train a new classifier using the labels for all the documents, iterate to convergence. This basic EM procedure works well when the data conform to the generative assumptions of the model. However, these assumptions are often violated in practice, and poor performance can result. Two extension improve classification accuracy (1) a weighting factor to modulate the contribution of the unlabeled data and (2) the use of multiple mixture components per class.

How can unlabeled data increase classification accuracy? Unlabelled data provide information about the joint probability distribution over words. Suppose that using only the labeled data we determine that documents containing the word `riot' belongs to the election violence class. If we use this fact to estimate the classification of the many unlabeled documents, we might find the word `bludgeon' occurs frequently with the word `riot' and we might construct a more accurate classifier that considers both `riot' and `bludgeon' as indicators of positive examples.


Given a training set ($x^{(i)}, y^{(i)}$) for $i=1 \dots n$, where $\theta$ is a paramter vector consiting of the values for all parameters $q(y)$ [the probability of seeing the label $y$] and $q_i(x \vert y)$ [the probability of attribute $j$ taking the value $x$ conditional on the underlying label being $y$] the log-likelihood function is:
\begin{equation}
\begin{split}
      L\left(\theta\right) & = \sum_{i=1}^{n} \log p\left(x^{(i)}, y^{(i)}\right) \\
           & =  \sum_{i=1}^{n} \log q(y^{(i)}) + \sum_{i=1}^{n} \sum_{j=1}^{d} \log q_j \left(x_j^{(i)} \vert y^{(i)}\right)
           \end{split}
\end{equation}
Idea 3: to find a set of keywords searches which returns $T$ - when you have a set of keywords which returns $T$ - adding new keywords will change $S$ but not reveal more cases of $T$ - so will not see large changes in individual word predictivity of class in Naive Bayes classifier.  encompasses the add search terms until revised data set $S\prime$ is not substantially changed by addition of new
When new search terms are added this changes the nature of $S$ - at each iteration new information added - consider changes to $S\prime$

<<>>=
names(king_stage2)<-c("rowname", "stage2_0", "stage2_1", "stage2_id")
names(keywords)<-c("rowname", "stage1_0", "stage1_1", "stage1_id")
left_join(king_stage2, keywords, by="rowname") %>%
  mutate(logit_stage1 =log(stage1_1/stage1_0), logit_stage2=log(stage2_1/stage2_0)) %>%
  ggplot(aes(logit_stage1, logit_stage2))+
  geom_point()
@


\end{document}
